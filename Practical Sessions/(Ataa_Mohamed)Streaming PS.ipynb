{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f97978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "201cee8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dbdfc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efac8040",
   "metadata": {},
   "source": [
    "### Create the schema of the streamed files (check the column names and types from the CSV files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8da5101e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('_c0', IntegerType(), True), StructField('Date', DateType(), True), StructField('Open', DoubleType(), True), StructField('High', DoubleType(), True), StructField('Low', DoubleType(), True), StructField('Close', DoubleType(), True), StructField('Adj Close', DoubleType(), True), StructField('Volume', IntegerType(), True)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = spark.read.csv('InputStreamFullData/', header=True, inferSchema=True).schema\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f50fbeb",
   "metadata": {},
   "source": [
    "### Create the dataframe by reading the stream using format \"csv\" and the schema you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d77671b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stream = spark.readStream.format('csv')\\\n",
    ".schema(s)\\\n",
    ".load('InputStreamFullData/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1e0e44",
   "metadata": {},
   "source": [
    "### Make sure the dataframe is streaming the files from the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc0fa13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stream.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9d68de",
   "metadata": {},
   "source": [
    "### Create a stream writer into memory and specify the query name \"stock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f40bf944",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df_stream.writeStream.format('memory')\\\n",
    ".outputMode('append')\\\n",
    ".queryName('stock')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a5f9a4",
   "metadata": {},
   "source": [
    "### Start the write stream and make sure it works (read all columns from the table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52bfb9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d98c6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/06 04:20:58 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1c472ead-2202-4a44-a41b-e02d4611ae0a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/10/06 04:20:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+---+-----+---------+------+\n",
      "|_c0|Date|Open|High|Low|Close|Adj Close|Volume|\n",
      "+---+----+----+----+---+-----+---------+------+\n",
      "+---+----+----+----+---+-----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = writer.start()\n",
    "spark.sql('select * from stock').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f699caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+---+-----+---------+------+\n",
      "|_c0|Date|Open|High|Low|Close|Adj Close|Volume|\n",
      "+---+----+----+----+---+-----+---------+------+\n",
      "+---+----+----+----+---+-----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from stock where 1 = 0').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "769e6e1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------+------------+------------+------------+------------+------+\n",
      "| _c0|      Date|        Open|        High|         Low|       Close|   Adj Close|Volume|\n",
      "+----+----------+------------+------------+------------+------------+------------+------+\n",
      "|NULL|      NULL|        NULL|        NULL|        NULL|        NULL|        NULL|  NULL|\n",
      "| 240|2000-12-05|26585.300781|27367.300781|26372.099609|27011.800781|25526.091797| 91019|\n",
      "| 241|2000-12-06|27011.800781|27509.400391|26798.599609|26869.699219|25391.804688|105791|\n",
      "| 242|2000-12-07|27011.800781|27011.800781|26478.699219|26656.400391|25190.236328| 40656|\n",
      "| 243|2000-12-08|26656.400391|27722.699219|26656.400391|27651.599609|26130.699219|149964|\n",
      "| 244|2000-12-11|27687.099609|     28860.0|27651.599609|28078.099609|26533.740234|159671|\n",
      "| 245|2000-12-12|28042.599609|28078.099609|27438.300781|27935.900391|26399.361328| 74560|\n",
      "| 246|2000-12-13|27651.599609|     29286.5|27651.599609|28469.099609|26903.234375|270385|\n",
      "| 247|2000-12-14|28469.099609|29784.099609|28291.300781|28362.400391| 26802.40625|256317|\n",
      "| 248|2000-12-15|28362.400391|28895.599609|27793.800781|27935.900391|26399.361328|108886|\n",
      "| 249|2000-12-18|     27580.5|     28433.5|27367.300781|28291.300781|26735.216797| 92848|\n",
      "| 250|2000-12-19|27722.699219|28788.900391|27651.599609|27651.599609|26130.699219|115779|\n",
      "| 251|2000-12-20|27402.800781|27793.800781|     27154.0|27722.699219|26197.890625| 99601|\n",
      "| 252|2000-12-21|27367.300781|27793.800781|27082.900391|27722.699219|26197.890625|129706|\n",
      "| 253|2000-12-22|27687.099609|     28007.0|27509.400391|     28007.0|26466.550781| 77233|\n",
      "| 254|2000-12-26|     27971.5|31987.699219|27473.900391|     28433.5|26869.591797|131535|\n",
      "| 255|2001-01-02|27367.300781|27367.300781|25945.599609|26656.400391|25190.236328| 64150|\n",
      "| 256|2001-01-03|26585.300781|26656.400391|26016.699219|26656.400391|25190.236328| 37421|\n",
      "| 257|2001-01-04|27367.300781|28078.099609|26905.199219|27082.900391|25593.277344|131254|\n",
      "| 258|2001-01-05|26940.800781|26940.800781|26194.400391|26585.300781|25123.048828| 90316|\n",
      "+----+----------+------------+------------+------------+------------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql('select * from stock')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16908fa6",
   "metadata": {},
   "source": [
    "### Remove the first row from the data (hint: drop the rows where ALL values are null), then add a new column \"diff\", which is the difference between high and low columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f2eaa25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- diff: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = df_stream.dropna(how = 'all').withColumn('diff', col('High')-col('Low'))\n",
    "df_1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f14581b",
   "metadata": {},
   "source": [
    "### Create a new write stream using the new generated dataframe and call the generate table \"modified_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9fb1be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_new = df_1.writeStream.format('memory')\\\n",
    ".outputMode('append')\\\n",
    ".queryName('modified_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "969acbbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/06 04:41:32 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-20be20f1-e9d8-4b93-88cc-d0e7abd7a719. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/10/06 04:41:32 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query_new = writer_new.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "916de634",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_new.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3bcedd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+---+-----+---------+------+----+\n",
      "|_c0|Date|Open|High|Low|Close|Adj Close|Volume|diff|\n",
      "+---+----+----+----+---+-----+---------+------+----+\n",
      "+---+----+----+----+---+-----+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from modified_data where 1 =0').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "453fbe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modified = spark.sql('select * from modified_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c3036",
   "metadata": {},
   "source": [
    "### Write the generated data into files instead of the memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6441b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df_1.writeStream.format('csv')\\\n",
    ".outputMode('append')\\\n",
    ".option('checkpointLocation','chck0')\\\n",
    ".option('path','MyOutputStream/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51f05e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/06 04:56:30 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query = writer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f07e3f",
   "metadata": {},
   "source": [
    "### Stop the query. Now, try reading the generated files into a normal dataframe\n",
    "- Create a schema and use it to read the data.\n",
    "- Show the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c4ca798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('240', IntegerType(), True), StructField('2000-12-05', DateType(), True), StructField('26585.300781', DoubleType(), True), StructField('27367.300781', DoubleType(), True), StructField('26372.099609', DoubleType(), True), StructField('27011.800781', DoubleType(), True), StructField('25526.091797', DoubleType(), True), StructField('91019', IntegerType(), True), StructField('995.201172000001', DoubleType(), True)])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_new = spark.read.csv('MyOutputStream/', header=True, inferSchema=True).schema\n",
    "s_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "184c47c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_new.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ec1321fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#spark.read.csv('InputStreamFullData/', header=True, inferSchema=True)\n",
    "df_stream_new = spark.read.csv('InputStreamFullData/',header=True, inferSchema=True)\n",
    "#.schema(s_new)\\\n",
    "#.load('MyOutputStream/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c71ca2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+------------+------------+------------+------------+------+\n",
      "| ID|      Date|        Open|        High|         Low|       Close|   Adj Close|Volume|\n",
      "+---+----------+------------+------------+------------+------------+------------+------+\n",
      "|240|2000-12-05|26585.300781|27367.300781|26372.099609|27011.800781|25526.091797| 91019|\n",
      "|241|2000-12-06|27011.800781|27509.400391|26798.599609|26869.699219|25391.804688|105791|\n",
      "|242|2000-12-07|27011.800781|27011.800781|26478.699219|26656.400391|25190.236328| 40656|\n",
      "|243|2000-12-08|26656.400391|27722.699219|26656.400391|27651.599609|26130.699219|149964|\n",
      "|244|2000-12-11|27687.099609|     28860.0|27651.599609|28078.099609|26533.740234|159671|\n",
      "|245|2000-12-12|28042.599609|28078.099609|27438.300781|27935.900391|26399.361328| 74560|\n",
      "|246|2000-12-13|27651.599609|     29286.5|27651.599609|28469.099609|26903.234375|270385|\n",
      "|247|2000-12-14|28469.099609|29784.099609|28291.300781|28362.400391| 26802.40625|256317|\n",
      "|248|2000-12-15|28362.400391|28895.599609|27793.800781|27935.900391|26399.361328|108886|\n",
      "|249|2000-12-18|     27580.5|     28433.5|27367.300781|28291.300781|26735.216797| 92848|\n",
      "|250|2000-12-19|27722.699219|28788.900391|27651.599609|27651.599609|26130.699219|115779|\n",
      "|251|2000-12-20|27402.800781|27793.800781|     27154.0|27722.699219|26197.890625| 99601|\n",
      "|252|2000-12-21|27367.300781|27793.800781|27082.900391|27722.699219|26197.890625|129706|\n",
      "|253|2000-12-22|27687.099609|     28007.0|27509.400391|     28007.0|26466.550781| 77233|\n",
      "|254|2000-12-26|     27971.5|31987.699219|27473.900391|     28433.5|26869.591797|131535|\n",
      "|255|2001-01-02|27367.300781|27367.300781|25945.599609|26656.400391|25190.236328| 64150|\n",
      "|256|2001-01-03|26585.300781|26656.400391|26016.699219|26656.400391|25190.236328| 37421|\n",
      "|257|2001-01-04|27367.300781|28078.099609|26905.199219|27082.900391|25593.277344|131254|\n",
      "|258|2001-01-05|26940.800781|26940.800781|26194.400391|26585.300781|25123.048828| 90316|\n",
      "|259|2001-01-08|26514.199219|27011.800781|26052.199219|26052.199219|24619.269531|102414|\n",
      "+---+----------+------------+------------+------------+------------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/06 05:09:44 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Date, Open, High, Low, Close, Adj Close, Volume\n",
      " Schema: _c0, Date, Open, High, Low, Close, Adj Close, Volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ataa/Spark/InputStreamFullData/KOSPI_STOCK_6.csv\n"
     ]
    }
   ],
   "source": [
    "df_stream_new1 = df_stream_new.withColumnRenamed('_c0','ID')\n",
    "df_stream_new1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4814d697",
   "metadata": {},
   "source": [
    "### Sort the dataframe based on the ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c51d91a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/06 05:09:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Date, Open, High, Low, Close, Adj Close, Volume\n",
      " Schema: _c0, Date, Open, High, Low, Close, Adj Close, Volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ataa/Spark/InputStreamFullData/KOSPI_STOCK_7.csv\n",
      "23/10/06 05:09:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Date, Open, High, Low, Close, Adj Close, Volume\n",
      " Schema: _c0, Date, Open, High, Low, Close, Adj Close, Volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ataa/Spark/InputStreamFullData/KOSPI_STOCK_6.csv\n",
      "23/10/06 05:09:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Date, Open, High, Low, Close, Adj Close, Volume\n",
      " Schema: _c0, Date, Open, High, Low, Close, Adj Close, Volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ataa/Spark/InputStreamFullData/KOSPI_STOCK_8.csv\n",
      "23/10/06 05:09:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Date, Open, High, Low, Close, Adj Close, Volume\n",
      " Schema: _c0, Date, Open, High, Low, Close, Adj Close, Volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ataa/Spark/InputStreamFullData/KOSPI_STOCK_0.csv\n",
      "23/10/06 05:09:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Date, Open, High, Low, Close, Adj Close, Volume\n",
      " Schema: _c0, Date, Open, High, Low, Close, Adj Close, Volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ataa/Spark/InputStreamFullData/KOSPI_STOCK_5.csv\n",
      "23/10/06 05:09:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Date, Open, High, Low, Close, Adj Close, Volume\n",
      " Schema: _c0, Date, Open, High, Low, Close, Adj Close, Volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ataa/Spark/InputStreamFullData/KOSPI_STOCK_10.csv\n",
      "23/10/06 05:09:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Date, Open, High, Low, Close, Adj Close, Volume\n",
      " Schema: _c0, Date, Open, High, Low, Close, Adj Close, Volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ataa/Spark/InputStreamFullData/KOSPI_STOCK_4.csv\n",
      "23/10/06 05:09:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Date, Open, High, Low, Close, Adj Close, Volume\n",
      " Schema: _c0, Date, Open, High, Low, Close, Adj Close, Volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ataa/Spark/InputStreamFullData/KOSPI_STOCK_3.csv\n",
      "23/10/06 05:09:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Date, Open, High, Low, Close, Adj Close, Volume\n",
      " Schema: _c0, Date, Open, High, Low, Close, Adj Close, Volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ataa/Spark/InputStreamFullData/KOSPI_STOCK_1.csv\n",
      "23/10/06 05:09:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Date, Open, High, Low, Close, Adj Close, Volume\n",
      " Schema: _c0, Date, Open, High, Low, Close, Adj Close, Volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ataa/Spark/InputStreamFullData/KOSPI_STOCK_9.csv\n",
      "23/10/06 05:09:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Date, Open, High, Low, Close, Adj Close, Volume\n",
      " Schema: _c0, Date, Open, High, Low, Close, Adj Close, Volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ataa/Spark/InputStreamFullData/KOSPI_STOCK_2.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+------------+------------+------------+------------+------+\n",
      "| ID|      Date|        Open|        High|         Low|       Close|   Adj Close|Volume|\n",
      "+---+----------+------------+------------+------------+------------+------------+------+\n",
      "|  0|2000-01-04|22817.900391|25696.800781|22817.900391|24879.300781|23510.880859|108745|\n",
      "|  1|2000-01-05|24523.900391|26229.900391|23670.900391|24417.300781|23074.294922|175990|\n",
      "|  2|2000-01-06|24381.699219|24666.099609|22746.800781|22817.900391|21562.865234| 71746|\n",
      "|  3|2000-01-07|     22036.0|24879.300781|     22036.0|23884.199219|22570.513672|120984|\n",
      "|  4|2000-01-10|24879.300781|25519.099609|23813.099609|24061.900391|22738.439453|151371|\n",
      "|  5|2000-01-11|     24168.5|     25021.5|23955.199219|24239.599609|22906.365234| 95943|\n",
      "|  6|2000-01-12|     24168.5|24452.800781|23457.599609|23670.900391|22368.947266| 61899|\n",
      "|  7|2000-01-13|23670.900391|24132.900391|23102.199219|23244.400391| 21965.90625| 57538|\n",
      "|  8|2000-01-14|23457.599609|     24168.5|22746.800781|23244.400391| 21965.90625| 84267|\n",
      "|  9|2000-01-17|22533.599609|23457.599609|22533.599609|23457.599609|22167.376953| 67807|\n",
      "| 10|2000-01-18|23457.599609|     23742.0|22746.800781|23422.099609|22133.832031| 27995|\n",
      "| 11|2000-01-19|22817.900391|23173.300781|     22036.0|     22036.0|20823.970703| 44173|\n",
      "| 12|2000-01-20|21325.099609|22000.400391|     20756.5|21680.599609|20488.117188| 47550|\n",
      "| 13|2000-01-21|21680.599609|22391.400391|20863.099609|21680.599609|20488.117188| 80750|\n",
      "| 14|2000-01-24|20969.699219|21822.699219|20969.699219|20969.699219|19816.320313| 79906|\n",
      "| 15|2000-01-25|20258.900391|20934.199219|     19548.0|20116.699219|19010.236328|170925|\n",
      "| 16|2000-01-26|20223.300781|20543.199219|19761.300781|     20330.0|19211.804688| 59929|\n",
      "| 17|2000-01-27|     20401.0|22746.800781|     20330.0|21040.800781|19883.511719|139132|\n",
      "| 18|2000-01-28|21431.800781|22107.099609|21040.800781|21964.900391|20756.783203| 78640|\n",
      "| 19|2000-01-31|21325.099609|21893.800781|     21183.0|21467.300781|20286.552734| 45861|\n",
      "+---+----------+------------+------------+------------+------------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalDFSorted = df_stream_new1.sort('ID')\n",
    "finalDFSorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e218d5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
